{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "\n",
    "path = \"../cv-cat-18/ca/validated.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_24625/3946300290.py:1: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_full = pd.read_csv(path,delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv(path,delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./macrodial.json\",'r',encoding='utf-8') as input: \n",
    "    macro_dial = json.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"grouped_accents\"] = df_full.apply(lambda row: macro_dial[row.accents] if macro_dial.get(row.accents, None) else \"None\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892545"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578278"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove repeats that occur in same macro-dialect\n",
    "df_full = df_full.drop_duplicates(subset=[\"sentence\",\"grouped_accents\"])\n",
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_full[df_full.grouped_accents=='valencià']\n",
    "df_cent = df_full[df_full.grouped_accents=='central']\n",
    "df_nord = df_full[df_full.grouped_accents=='nord']\n",
    "df_bal = df_full[df_full.grouped_accents=='balear']\n",
    "df_no = df_full[df_full.grouped_accents=='nord-occidental']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval = df_val.sample(n=2486)\n",
    "cent_eval = df_cent.sample(n=2486)\n",
    "nord_eval = df_nord.sample(n=2486)\n",
    "bal_eval = df_bal.sample(n=2486)\n",
    "no_eval = df_no.sample(n=2486)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain',\n",
       "       'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant',\n",
       "       'locale', 'segment', 'grouped_accents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_eval = pd.concat([val_eval,cent_eval,nord_eval,bal_eval,no_eval],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../cv-cat-18/ca/\"\n",
    "full_eval.to_csv(out_path+\"eval_balanced.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578278"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = df_full.drop(pd.concat([val_eval,cent_eval,nord_eval,bal_eval,no_eval]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1565848"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the lengths of the train_val set correspond to what they should after removing eval\n",
    "assert (len(df_full)-len(train_val_df)) == len(full_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_domain</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>variant</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "      <th>grouped_accents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376288</th>\n",
       "      <td>1d54d194cff4675f590e0e73e98f6392358891d09dd19b...</td>\n",
       "      <td>common_voice_ca_19031942.mp3</td>\n",
       "      <td>0f935e0f31a3462e7d934c568af5054def5f87595f7879...</td>\n",
       "      <td>La mort.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fifties</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>valencià</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valencià</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749002</th>\n",
       "      <td>e7847a5814b865bc043600fee7d810b9815da389278fdf...</td>\n",
       "      <td>common_voice_ca_23910869.mp3</td>\n",
       "      <td>be7816b979462943ae67b7459b5356e3f3d577210a2ab4...</td>\n",
       "      <td>Ocasionalment reaparegué el seu nom, més, enca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>teens</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>valencià</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valencià</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048366</th>\n",
       "      <td>dafd89491990553f5e22021f96344b3bc92be6a419c919...</td>\n",
       "      <td>common_voice_ca_20131812.mp3</td>\n",
       "      <td>1d1b58b48145e454ded466211cb4d29844b1f2b4eecbf9...</td>\n",
       "      <td>Això va permetre la creació dels primers canal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>valencià</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valencià</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529614</th>\n",
       "      <td>892bf89bd3a008a7d982de0d278349e654c713efacf965...</td>\n",
       "      <td>common_voice_ca_38406447.mp3</td>\n",
       "      <td>aff8538286761070459fa83f159bd919d98b2390e973c6...</td>\n",
       "      <td>Actualment és un mineral aprovat per l'Associa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fourties</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>valencià,La Vall d'Albaida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valencià</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730340</th>\n",
       "      <td>7d19dccf48114d3ec00c45fe80581300faca042157d6c9...</td>\n",
       "      <td>common_voice_ca_20757891.mp3</td>\n",
       "      <td>358cefe95803b4cb70ff108b545363767d944e17e6ee4d...</td>\n",
       "      <td>S'utilitza sobre animacions publicitàries, cur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>male_masculine</td>\n",
       "      <td>valencià</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valencià</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 client_id  \\\n",
       "376288   1d54d194cff4675f590e0e73e98f6392358891d09dd19b...   \n",
       "749002   e7847a5814b865bc043600fee7d810b9815da389278fdf...   \n",
       "1048366  dafd89491990553f5e22021f96344b3bc92be6a419c919...   \n",
       "1529614  892bf89bd3a008a7d982de0d278349e654c713efacf965...   \n",
       "730340   7d19dccf48114d3ec00c45fe80581300faca042157d6c9...   \n",
       "\n",
       "                                 path  \\\n",
       "376288   common_voice_ca_19031942.mp3   \n",
       "749002   common_voice_ca_23910869.mp3   \n",
       "1048366  common_voice_ca_20131812.mp3   \n",
       "1529614  common_voice_ca_38406447.mp3   \n",
       "730340   common_voice_ca_20757891.mp3   \n",
       "\n",
       "                                               sentence_id  \\\n",
       "376288   0f935e0f31a3462e7d934c568af5054def5f87595f7879...   \n",
       "749002   be7816b979462943ae67b7459b5356e3f3d577210a2ab4...   \n",
       "1048366  1d1b58b48145e454ded466211cb4d29844b1f2b4eecbf9...   \n",
       "1529614  aff8538286761070459fa83f159bd919d98b2390e973c6...   \n",
       "730340   358cefe95803b4cb70ff108b545363767d944e17e6ee4d...   \n",
       "\n",
       "                                                  sentence sentence_domain  \\\n",
       "376288                                            La mort.             NaN   \n",
       "749002   Ocasionalment reaparegué el seu nom, més, enca...             NaN   \n",
       "1048366  Això va permetre la creació dels primers canal...             NaN   \n",
       "1529614  Actualment és un mineral aprovat per l'Associa...             NaN   \n",
       "730340   S'utilitza sobre animacions publicitàries, cur...             NaN   \n",
       "\n",
       "         up_votes  down_votes       age          gender  \\\n",
       "376288          3           0   fifties  male_masculine   \n",
       "749002          2           0     teens  male_masculine   \n",
       "1048366         3           0  twenties  male_masculine   \n",
       "1529614         3           0  fourties  male_masculine   \n",
       "730340          3           0  thirties  male_masculine   \n",
       "\n",
       "                            accents variant locale segment grouped_accents  \n",
       "376288                     valencià     NaN     ca     NaN        valencià  \n",
       "749002                     valencià     NaN     ca     NaN        valencià  \n",
       "1048366                    valencià     NaN     ca     NaN        valencià  \n",
       "1529614  valencià,La Vall d'Albaida     NaN     ca     NaN        valencià  \n",
       "730340                     valencià     NaN     ca     NaN        valencià  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([val_eval,cent_eval,nord_eval,bal_eval,no_eval]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(376288 in list(train_val_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(730340 in list(train_val_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df.to_csv(out_path+\"train_dev_full.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First train/dev split: \n",
    "* 100% Central Catalan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/3390739693.py:6: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/3390739693.py:6: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/3390739693.py:6: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/3390739693.py:6: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/3390739693.py:6: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "# for this split, train has 99455 (central) samples; dev has 12430 samples\n",
    "#Obtan five random samples:\n",
    "\n",
    "for i in range(5):\n",
    "    #Get a new version of the train/dev df every iteration \n",
    "    temp =  pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "\n",
    "    #isolate central samples\n",
    "    train_dev_cen_full = temp[temp.grouped_accents=='central']\n",
    "\n",
    "    #Get the dev split for this sample\n",
    "    central_dev_100 = train_dev_cen_full.sample(n=12430)\n",
    "\n",
    "    #remove the samples dev from this version of the df\n",
    "    no_dev = train_dev_cen_full.drop(central_dev_100.index)\n",
    "\n",
    "    #get the train samples\n",
    "    central_train_100 = no_dev.sample(n=99455)\n",
    "\n",
    "    #Save both these \n",
    "    central_dev_100.to_csv(f\"../samples/samp_0{i+1}/dev_100.tsv\",sep='\\t',index=False)\n",
    "    central_train_100.to_csv(f\"../samples/samp_0{i+1}/train_100.tsv\",sep='\\t',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Train/dev split:\n",
    "* 80% Central (79564 train samples, 9944 dev samples)\n",
    "* 5% each of Valencià, nord, nord-occidental, and balear (4972 train samples each, 621 dev samples each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2578635210.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2578635210.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2578635210.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2578635210.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2578635210.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "#Obtan five random samples:\n",
    "\n",
    "for i in range(5):\n",
    "    #Get a new version of the train/dev df every iteration \n",
    "    temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "\n",
    "    #isolate central samples\n",
    "    train_dev_cen_full = temp[temp.grouped_accents=='central']\n",
    "\n",
    "    #Get the dev split for this sample\n",
    "    central_dev_80 = train_dev_cen_full.sample(n=9944)\n",
    "\n",
    "    #remove the samples dev from this version of the df\n",
    "    no_dev_cen = train_dev_cen_full.drop(central_dev_80.index)\n",
    "\n",
    "    #get the train samples\n",
    "    central_train_80 = no_dev_cen.sample(n=79564)\n",
    "\n",
    "    #isolate valencian samples\n",
    "    train_dev_val_full = temp[temp.grouped_accents=='valencià']\n",
    "    valencia_dev_80 = train_dev_val_full.sample(n=621)\n",
    "    no_dev_val = train_dev_val_full.drop(valencia_dev_80.index)\n",
    "    valencia_train_80 = no_dev_val.sample(n=4972)\n",
    "\n",
    "    #isolate nord samples\n",
    "    train_dev_nor_full = temp[temp.grouped_accents=='nord']\n",
    "    nord_dev_80 = train_dev_nor_full.sample(n=621)\n",
    "    no_dev_nor = train_dev_nor_full.drop(nord_dev_80.index)\n",
    "    nord_train_80 = no_dev_nor.sample(n=4972)   \n",
    "\n",
    "    #isolate nord-occidental samples \n",
    "    train_dev_noc_full = temp[temp.grouped_accents=='nord-occidental']\n",
    "    nord_oc_dev_80 = train_dev_noc_full.sample(n=621)\n",
    "    no_dev_noc = train_dev_noc_full.drop(nord_oc_dev_80.index)\n",
    "    nord_oc_train_80 = no_dev_noc.sample(n=4972)   \n",
    "\n",
    "    #isolate balear samples\n",
    "    train_dev_bal_full = temp[temp.grouped_accents=='balear']\n",
    "    bal_dev_80 = train_dev_bal_full.sample(n=621)\n",
    "    no_dev_bal = train_dev_bal_full.drop(bal_dev_80.index)\n",
    "    bal_train_80 = no_dev_bal.sample(n=4972)\n",
    "\n",
    "    #Concatenate everything and save\n",
    "    full_train_80 = pd.concat([central_train_80,valencia_train_80,nord_train_80,nord_oc_train_80,bal_train_80],ignore_index=True)\n",
    "    full_dev_80 = pd.concat([central_dev_80,valencia_dev_80,nord_dev_80,nord_oc_dev_80,bal_dev_80],ignore_index=True)      \n",
    "\n",
    "    #Save both these \n",
    "    full_train_80.to_csv(f\"../samples/samp_0{i+1}/train_80.tsv\",sep='\\t',index=False)\n",
    "    full_dev_80.to_csv(f\"../samples/samp_0{i+1}/dev_80.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4972"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_80[full_train_80.grouped_accents==\"nord\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Train/dev split:\n",
    "* 50% Central (49727 train samples, 6215 dev samples)\n",
    "* 12.5% each of Valencià, nord, nord-occidental, and balear (12431 train samples each, 1553 dev samples each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/1885040126.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/1885040126.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/1885040126.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/1885040126.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/1885040126.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "#Obtan five random samples:\n",
    "\n",
    "for i in range(5):\n",
    "    #Get a new version of the train/dev df every iteration \n",
    "    temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "\n",
    "    #isolate central samples\n",
    "    train_dev_cen_full = temp[temp.grouped_accents=='central']\n",
    "\n",
    "    #Get the dev split for this sample\n",
    "    central_dev_50 = train_dev_cen_full.sample(n=6215)\n",
    "\n",
    "    #remove the samples dev from this version of the df\n",
    "    no_dev_cen = train_dev_cen_full.drop(central_dev_50.index)\n",
    "\n",
    "    #get the train samples\n",
    "    central_train_50 = no_dev_cen.sample(n=49727)\n",
    "\n",
    "    #isolate valencian samples\n",
    "    train_dev_val_full = temp[temp.grouped_accents=='valencià']\n",
    "    valencia_dev_50 = train_dev_val_full.sample(n=1553)\n",
    "    no_dev_val = train_dev_val_full.drop(valencia_dev_50.index)\n",
    "    valencia_train_50 = no_dev_val.sample(n=12431)\n",
    "\n",
    "    #isolate nord samples\n",
    "    train_dev_nor_full = temp[temp.grouped_accents=='nord']\n",
    "    nord_dev_50 = train_dev_nor_full.sample(n=1553)\n",
    "    no_dev_nor = train_dev_nor_full.drop(nord_dev_50.index)\n",
    "    nord_train_50 = no_dev_nor.sample(n=12431)   \n",
    "\n",
    "    #isolate nord-occidental samples \n",
    "    train_dev_noc_full = temp[temp.grouped_accents=='nord-occidental']\n",
    "    nord_oc_dev_50 = train_dev_noc_full.sample(n=1553)\n",
    "    no_dev_noc = train_dev_noc_full.drop(nord_oc_dev_50.index)\n",
    "    nord_oc_train_50 = no_dev_noc.sample(n=12431)   \n",
    "\n",
    "    #isolate balear samples\n",
    "    train_dev_bal_full = temp[temp.grouped_accents=='balear']\n",
    "    bal_dev_50 = train_dev_bal_full.sample(n=1553)\n",
    "    no_dev_bal = train_dev_bal_full.drop(bal_dev_50.index)\n",
    "    bal_train_50 = no_dev_bal.sample(n=12431)\n",
    "\n",
    "    #Concatenate everything and save\n",
    "    full_train_50 = pd.concat([central_train_50,valencia_train_50,nord_train_50,nord_oc_train_50,bal_train_50],ignore_index=True)\n",
    "    full_dev_50 = pd.concat([central_dev_50,valencia_dev_50,nord_dev_50,nord_oc_dev_50,bal_dev_50],ignore_index=True)      \n",
    "\n",
    "    #Save both these \n",
    "    full_train_50.to_csv(f\"../samples/samp_0{i+1}/train_50.tsv\",sep='\\t',index=False)\n",
    "    full_dev_50.to_csv(f\"../samples/samp_0{i+1}/dev_50.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_50[full_train_50.grouped_accents==\"balear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Train/dev split:\n",
    "* All dialects balanced: 2486 dev samples of each, 19891 train samples of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2304751542.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2304751542.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2304751542.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2304751542.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
      "/var/folders/2d/7d3c_x1x5mb40mc1sgr14p740000gn/T/ipykernel_8697/2304751542.py:5: DtypeWarning: Columns (4,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n"
     ]
    }
   ],
   "source": [
    "#Obtan five random samples:\n",
    "\n",
    "for i in range(5):\n",
    "    #Get a new version of the train/dev df every iteration \n",
    "    temp = pd.read_csv(\"../samples/train_dev_full.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "\n",
    "    #isolate central samples\n",
    "    train_dev_cen_full = temp[temp.grouped_accents=='central']\n",
    "\n",
    "    #Get the dev split for this sample\n",
    "    central_dev_20 = train_dev_cen_full.sample(n=2486)\n",
    "\n",
    "    #remove the samples dev from this version of the df\n",
    "    no_dev_cen = train_dev_cen_full.drop(central_dev_20.index)\n",
    "\n",
    "    #get the train samples\n",
    "    central_train_20 = no_dev_cen.sample(n=19891)\n",
    "\n",
    "    #isolate valencian samples\n",
    "    train_dev_val_full = temp[temp.grouped_accents=='valencià']\n",
    "    valencia_dev_20 = train_dev_val_full.sample(n=2486)\n",
    "    no_dev_val = train_dev_val_full.drop(valencia_dev_20.index)\n",
    "    valencia_train_20 = no_dev_val.sample(n=19891)\n",
    "\n",
    "    #isolate nord samples\n",
    "    train_dev_nor_full = temp[temp.grouped_accents=='nord']\n",
    "    nord_dev_20 = train_dev_nor_full.sample(n=1486)\n",
    "    no_dev_nor = train_dev_nor_full.drop(nord_dev_20.index)\n",
    "    nord_train_20 = no_dev_nor.sample(n=19891)   \n",
    "\n",
    "    #isolate nord-occidental samples \n",
    "    train_dev_noc_full = temp[temp.grouped_accents=='nord-occidental']\n",
    "    nord_oc_dev_20 = train_dev_noc_full.sample(n=2486)\n",
    "    no_dev_noc = train_dev_noc_full.drop(nord_oc_dev_20.index)\n",
    "    nord_oc_train_20 = no_dev_noc.sample(n=19891)   \n",
    "\n",
    "    #isolate balear samples\n",
    "    train_dev_bal_full = temp[temp.grouped_accents=='balear']\n",
    "    bal_dev_20 = train_dev_bal_full.sample(n=2486)\n",
    "    no_dev_bal = train_dev_bal_full.drop(bal_dev_20.index)\n",
    "    bal_train_20 = no_dev_bal.sample(n=19891)\n",
    "\n",
    "    #Concatenate everything and save\n",
    "    full_train_20 = pd.concat([central_train_20,valencia_train_20,nord_train_20,nord_oc_train_20,bal_train_20],ignore_index=True)\n",
    "    full_dev_20 = pd.concat([central_dev_20,valencia_dev_20,nord_dev_20,nord_oc_dev_20,bal_dev_20],ignore_index=True)      \n",
    "\n",
    "    #Save both these \n",
    "    full_train_20.to_csv(f\"../samples/samp_0{i+1}/train_20.tsv\",sep='\\t',index=False)\n",
    "    full_dev_20.to_csv(f\"../samples/samp_0{i+1}/dev_20.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19891"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_20[full_train_20.grouped_accents==\"valencià\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Adversarial samples off the eval set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = pd.read_csv(\"../eval_balanced.tsv\",delimiter=\"\\t\", escapechar=\"\\\\\",quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12430"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check there are the correct amount of samples (12430)\n",
    "len(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eval = eval[eval.grouped_accents=='valencià']\n",
    "central_eval = eval[eval.grouped_accents=='central']\n",
    "nord_eval=eval[eval.grouped_accents=='nord']\n",
    "bal_eval = eval[eval.grouped_accents=='balear']\n",
    "no_eval = eval[eval.grouped_accents=='nord-occidental']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cw = val_eval.sample(n=50)\n",
    "central_cw = central_eval.sample(n=50)\n",
    "nord_cw = nord_eval.sample(n=50)\n",
    "bal_cw = bal_eval.sample(n=50)\n",
    "no_cw = no_eval.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate everything and save\n",
    "full_cw_set = pd.concat([val_cw,central_cw,nord_cw,bal_cw,no_cw],ignore_index=True)\n",
    "len(full_cw_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "full_cw_set.to_csv(\"../attack_samp_balanced.tsv\",sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
